{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Approach_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cjl5RHgMNBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UsbeJyPFTDB",
        "outputId": "0f766259-0403-46ff-fa3b-816b7b0d4146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt"
      ],
      "metadata": {
        "id": "dv5Cy7kHd6Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "class ClassificationHead(torch.nn.Module):\n",
        "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
        "\n",
        "    def __init__(self, class_size, embed_size):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.class_size = class_size\n",
        "        self.embed_size = embed_size\n",
        "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        logits = self.mlp(hidden_state)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Ot3WoIwD5c_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfSenKa05Kus"
      },
      "outputs": [],
      "source": [
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from torchtext import data as torchtext_data\n",
        "from torchtext import datasets\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "EPSILON = 1e-10\n",
        "example_sentence = \"This is incredible! I love it, this is the best chicken I have ever had.\"\n",
        "max_length_seq = 1000\n",
        "\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    \"\"\"Transformer encoder followed by a Classification Head\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            class_size=None,\n",
        "            pretrained_model=\"gpt2-medium\",\n",
        "            classifier_head=None,\n",
        "            cached_mode=False,\n",
        "            device='cpu'\n",
        "    ):\n",
        "        super(Discriminator, self).__init__()\n",
        "        if pretrained_model.startswith(\"gpt2\"):\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "            self.encoder = GPT2LMHeadModel.from_pretrained(pretrained_model)\n",
        "            self.embed_size = self.encoder.transformer.config.hidden_size\n",
        "        elif pretrained_model.startswith(\"bert\"):\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
        "            self.encoder = BertModel.from_pretrained(pretrained_model)\n",
        "            self.embed_size = self.encoder.config.hidden_size\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"{} model not yet supported\".format(pretrained_model)\n",
        "            )\n",
        "        if classifier_head:\n",
        "            self.classifier_head = classifier_head\n",
        "        else:\n",
        "            if not class_size:\n",
        "                raise ValueError(\"must specify class_size\")\n",
        "            self.classifier_head = ClassificationHead(\n",
        "                class_size=class_size,\n",
        "                embed_size=self.embed_size\n",
        "            )\n",
        "        self.cached_mode = cached_mode\n",
        "        self.device = device\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.classifier_head\n",
        "\n",
        "    def train_custom(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.classifier_head.train()\n",
        "\n",
        "    def avg_representation(self, x):\n",
        "        mask = x.ne(0).unsqueeze(2).repeat(\n",
        "            1, 1, self.embed_size\n",
        "        ).float().to(self.device).detach()\n",
        "        if hasattr(self.encoder, 'transformer'):\n",
        "            # for gpt2\n",
        "            hidden, _ = self.encoder.transformer(x)\n",
        "        else:\n",
        "            # for bert\n",
        "            hidden, _ = self.encoder(x)\n",
        "        # print(\"Debug1\")\n",
        "        # print(hidden, mask)\n",
        "        masked_hidden = hidden * mask\n",
        "        avg_hidden = torch.sum(masked_hidden, dim=1) / (\n",
        "                torch.sum(mask, dim=1).detach() + EPSILON\n",
        "        )\n",
        "        return avg_hidden\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.cached_mode:\n",
        "            avg_hidden = x.to(self.device)\n",
        "        else:\n",
        "            avg_hidden = self.avg_representation(x.to(self.device))\n",
        "\n",
        "        logits = self.classifier_head(avg_hidden)\n",
        "        probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def predict(self, input_sentence):\n",
        "        input_t = self.tokenizer.encode(input_sentence)\n",
        "        input_t = torch.tensor([input_t], dtype=torch.long, device=self.device)\n",
        "        if self.cached_mode:\n",
        "            input_t = self.avg_representation(input_t)\n",
        "\n",
        "        log_probs = self(input_t).data.cpu().numpy().flatten().tolist()\n",
        "        prob = [math.exp(log_prob) for log_prob in log_probs]\n",
        "        return prob\n",
        "\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"Reads source and target sequences from txt files.\"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
        "        data = {}\n",
        "        data[\"X\"] = self.X[index]\n",
        "        data[\"y\"] = self.y[index]\n",
        "        return data\n",
        "\n",
        "\n",
        "def collate_fn(data):\n",
        "    def pad_sequences(sequences):\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "\n",
        "        padded_sequences = torch.zeros(\n",
        "            len(sequences),\n",
        "            max(lengths)\n",
        "        ).long()  # padding value = 0\n",
        "\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_sequences[i, :end] = seq[:end]\n",
        "\n",
        "        return padded_sequences, lengths\n",
        "\n",
        "    item_info = {}\n",
        "    for key in data[0].keys():\n",
        "        item_info[key] = [d[key] for d in data]\n",
        "\n",
        "    x_batch, _ = pad_sequences(item_info[\"X\"])\n",
        "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "def cached_collate_fn(data):\n",
        "    item_info = {}\n",
        "    for key in data[0].keys():\n",
        "        item_info[key] = [d[key] for d in data]\n",
        "\n",
        "    x_batch = torch.cat(item_info[\"X\"], 0)\n",
        "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "def train_epoch(data_loader, discriminator, optimizer,\n",
        "                epoch=0, log_interval=10, device='cpu'):\n",
        "    samples_so_far = 0\n",
        "    discriminator.train_custom()\n",
        "    for batch_idx, (input_t, target_t) in enumerate(data_loader):\n",
        "        input_t, target_t = input_t.to(device), target_t.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_t = discriminator(input_t)\n",
        "        loss = F.nll_loss(output_t, target_t)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        samples_so_far += len(input_t)\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch + 1,\n",
        "                    samples_so_far, len(data_loader.dataset),\n",
        "                    100 * samples_so_far / len(data_loader.dataset), loss.item()\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def evaluate_performance(data_loader, discriminator, device='cpu'):\n",
        "    discriminator.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for input_t, target_t in data_loader:\n",
        "            input_t, target_t = input_t.to(device), target_t.to(device)\n",
        "            output_t = discriminator(input_t)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output_t, target_t, reduction=\"sum\").item()\n",
        "            # get the index of the max log-probability\n",
        "            pred_t = output_t.argmax(dim=1, keepdim=True)\n",
        "            correct += pred_t.eq(target_t.view_as(pred_t)).sum().item()\n",
        "\n",
        "    test_loss /= len(data_loader.dataset)\n",
        "    accuracy = correct / len(data_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"Performance on test set: \"\n",
        "        \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
        "            test_loss, correct, len(data_loader.dataset),\n",
        "            100. * accuracy\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "def predict(input_sentence, model, classes, cached=False, device='cpu'):\n",
        "    input_t = model.tokenizer.encode(input_sentence)\n",
        "    input_t = torch.tensor([input_t], dtype=torch.long, device=device)\n",
        "    if cached:\n",
        "        input_t = model.avg_representation(input_t)\n",
        "\n",
        "    log_probs = model(input_t).data.cpu().numpy().flatten().tolist()\n",
        "    print(\"Input sentence:\", input_sentence)\n",
        "    print(\"Predictions:\", \", \".join(\n",
        "        \"{}: {:.4f}\".format(c, math.exp(log_prob)) for c, log_prob in\n",
        "        zip(classes, log_probs)\n",
        "    ))\n",
        "\n",
        "\n",
        "def get_cached_data_loader(dataset, batch_size, discriminator,\n",
        "                           shuffle=False, device='cpu'):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              collate_fn=collate_fn)\n",
        "\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for batch_idx, (x, y) in enumerate(tqdm(data_loader, ascii=True)):\n",
        "        with torch.no_grad():\n",
        "            x = x.to(device)\n",
        "            avg_rep = discriminator.avg_representation(x).cpu().detach()\n",
        "            avg_rep_list = torch.unbind(avg_rep.unsqueeze(1))\n",
        "            xs += avg_rep_list\n",
        "            ys += y.cpu().numpy().tolist()\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=Dataset(xs, ys),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=cached_collate_fn)\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def get_idx2class(dataset_fp):\n",
        "    print(\"In get_idx2class\")\n",
        "    classes = set()\n",
        "   \n",
        "    dummy = pd.read_csv(dataset_fp, header=None)\n",
        "    classes = dummy[0].unique()\n",
        "\n",
        "\n",
        "    return sorted(classes)\n",
        "\n",
        "def get_generic_dataset(dataset_fp, tokenizer, device,\n",
        "                        idx2class=None, add_eos_token=False):\n",
        "    print(\"In get_generic_dataset\")\n",
        "    if not idx2class:\n",
        "        idx2class = get_idx2class(dataset_fp)\n",
        "    class2idx = {c: i for i, c in enumerate(idx2class)}\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "  \n",
        "    dummy = pd.read_csv(dataset_fp, header=None)\n",
        "    for index, row in dummy.iterrows():\n",
        "      # if row:\n",
        "      label = row[0]\n",
        "      text = row[1]\n",
        "\n",
        "      try:\n",
        "          seq = tokenizer.encode(text)\n",
        "          if (len(seq) < max_length_seq):\n",
        "              if add_eos_token:\n",
        "                  seq = [50256] + seq\n",
        "              seq = torch.tensor(\n",
        "                  seq,\n",
        "                  device=device,\n",
        "                  dtype=torch.long\n",
        "              )\n",
        "\n",
        "          else:\n",
        "              print(\n",
        "                  \"Line {} is longer than maximum length {}\".format(\n",
        "                      i, max_length_seq\n",
        "                  ))\n",
        "              continue\n",
        "\n",
        "          x.append(seq)\n",
        "          y.append(class2idx[label])\n",
        "\n",
        "      except:\n",
        "          print(\"Error tokenizing line {}, skipping it\".format(i))\n",
        "          pass\n",
        "\n",
        "    return Dataset(x, y)\n",
        "\n",
        "\n",
        "def train_discriminator(\n",
        "        dataset,\n",
        "        dataset_fp=None,\n",
        "        pretrained_model=\"gpt2-medium\",\n",
        "        epochs=10,\n",
        "        learning_rate=0.0001,\n",
        "        batch_size=64,\n",
        "        log_interval=10,\n",
        "        save_model=False,\n",
        "        cached=False,\n",
        "        no_cuda=False,\n",
        "        output_fp='.'\n",
        "):\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "    print(\"Device: \", device)\n",
        "    add_eos_token = pretrained_model.startswith(\"gpt2\")\n",
        "\n",
        "    if save_model:\n",
        "        if not os.path.exists(output_fp):\n",
        "            os.makedirs(output_fp)\n",
        "    classifier_head_meta_fp = os.path.join(\n",
        "        output_fp, \"{}_classifier_head_meta.json\".format(dataset)\n",
        "    )\n",
        "    classifier_head_fp_pattern = os.path.join(\n",
        "        output_fp, \"{}_classifier_head_epoch\".format(dataset) + \"_{}.pt\"\n",
        "    )\n",
        "\n",
        "    print(\"Preprocessing {} dataset...\".format(dataset))\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    if dataset_fp is None:\n",
        "        raise ValueError(\"When generic dataset is selected, \"\n",
        "                          \"dataset_fp needs to be specified aswell.\")\n",
        "\n",
        "    idx2class = get_idx2class(dataset_fp)\n",
        "\n",
        "    discriminator = Discriminator(\n",
        "        class_size=len(idx2class),\n",
        "        pretrained_model=pretrained_model,\n",
        "        cached_mode=cached,\n",
        "        device=device\n",
        "    ).to(device)\n",
        "\n",
        "    full_dataset = get_generic_dataset(\n",
        "        dataset_fp, discriminator.tokenizer, device,\n",
        "        idx2class=idx2class, add_eos_token=add_eos_token\n",
        "    )\n",
        "    train_size = int(0.9 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset,\n",
        "        [train_size, test_size]\n",
        "    )\n",
        "\n",
        "    discriminator_meta = {\n",
        "        \"class_size\": len(idx2class),\n",
        "        \"embed_size\": discriminator.embed_size,\n",
        "        \"pretrained_model\": pretrained_model,\n",
        "        \"class_vocab\": {c: i for i, c in enumerate(idx2class)},\n",
        "        \"default_class\": 0,\n",
        "    }\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Preprocessed {} data points\".format(\n",
        "        len(train_dataset) + len(test_dataset))\n",
        "    )\n",
        "    print(\"Data preprocessing took: {:.3f}s\".format(end - start))\n",
        "\n",
        "    if cached:\n",
        "        print(\"Building representation cache...\")\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        train_loader = get_cached_data_loader(\n",
        "            train_dataset, batch_size, discriminator,\n",
        "            shuffle=True, device=device\n",
        "        )\n",
        "\n",
        "        test_loader = get_cached_data_loader(\n",
        "            test_dataset, batch_size, discriminator, device=device\n",
        "        )\n",
        "\n",
        "        end = time.time()\n",
        "        print(\"Building representation cache took: {:.3f}s\".format(end - start))\n",
        "\n",
        "    else:\n",
        "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   collate_fn=collate_fn)\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  collate_fn=collate_fn)\n",
        "\n",
        "    if save_model:\n",
        "        with open(classifier_head_meta_fp, \"w\") as meta_file:\n",
        "            json.dump(discriminator_meta, meta_file)\n",
        "\n",
        "    optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        print(\"\\nEpoch\", epoch + 1)\n",
        "\n",
        "        train_epoch(\n",
        "            discriminator=discriminator,\n",
        "            data_loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            epoch=epoch,\n",
        "            log_interval=log_interval,\n",
        "            device=device\n",
        "        )\n",
        "        test_loss, test_accuracy = evaluate_performance(\n",
        "            data_loader=test_loader,\n",
        "            discriminator=discriminator,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        end = time.time()\n",
        "        print(\"Epoch took: {:.3f}s\".format(end - start))\n",
        "\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(\"\\nExample prediction\")\n",
        "        predict(example_sentence, discriminator, idx2class,\n",
        "                cached=cached, device=device)\n",
        "\n",
        "        if save_model:\n",
        "            # torch.save(discriminator.state_dict(),\n",
        "            #           \"{}_discriminator_{}.pt\".format(\n",
        "            #               args.dataset, epoch + 1\n",
        "            #               ))\n",
        "            torch.save(discriminator.get_classifier().state_dict(),\n",
        "                       classifier_head_fp_pattern.format(epoch + 1))\n",
        "\n",
        "    min_loss = float(\"inf\")\n",
        "    min_loss_epoch = 0\n",
        "    max_acc = 0.0\n",
        "    max_acc_epoch = 0\n",
        "    print(\"Test performance per epoch\")\n",
        "    print(\"epoch\\tloss\\tacc\")\n",
        "    for e, (loss, acc) in enumerate(zip(test_losses, test_accuracies)):\n",
        "        print(\"{}\\t{}\\t{}\".format(e + 1, loss, acc))\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            min_loss_epoch = e + 1\n",
        "        if acc > max_acc:\n",
        "            max_acc = acc\n",
        "            max_acc_epoch = e + 1\n",
        "    print(\"Min loss: {} - Epoch: {}\".format(min_loss, min_loss_epoch))\n",
        "    print(\"Max acc: {} - Epoch: {}\".format(max_acc, max_acc_epoch))\n",
        "\n",
        "    return discriminator, discriminator_meta\n",
        "\n",
        "\n",
        "def load_classifier_head(weights_path, meta_path, device='cpu'):\n",
        "    with open(meta_path, 'r', encoding=\"utf8\") as f:\n",
        "        meta_params = json.load(f)\n",
        "    classifier_head = ClassificationHead(\n",
        "        class_size=meta_params['class_size'],\n",
        "        embed_size=meta_params['embed_size']\n",
        "    ).to(device)\n",
        "    classifier_head.load_state_dict(\n",
        "        torch.load(weights_path, map_location=device))\n",
        "    classifier_head.eval()\n",
        "    return classifier_head, meta_params\n",
        "\n",
        "\n",
        "def load_discriminator(weights_path, meta_path, device='cpu'):\n",
        "    classifier_head, meta_param = load_classifier_head(\n",
        "        weights_path, meta_path, device\n",
        "    )\n",
        "    discriminator =  Discriminator(\n",
        "        pretrained_model=meta_param['pretrained_model'],\n",
        "        classifier_head=classifier_head,\n",
        "        cached_mode=False,\n",
        "        device=device\n",
        "    )\n",
        "    return discriminator, meta_param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_discriminator(dataset=\"generic\",dataset_fp=\"/content/gdrive/MyDrive/cs685/PPLM/party_data.csv\", \n",
        "                    save_model=True, output_fp = \"/content/gdrive/MyDrive/cs685/classifier_model/party/\",\n",
        "                    batch_size=32, epochs = 5)"
      ],
      "metadata": {
        "id": "qxBDR-2B5umc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
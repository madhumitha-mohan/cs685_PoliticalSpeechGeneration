{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Approach_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfcRMHrVkQQN",
        "outputId": "39629833-a1f1-4d19-9fb4-7152e3859f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (3.0.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNBSgpJun_qC",
        "outputId": "b130cadb-4e6a-438e-8d0c-77d550ce6d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir(/content/gdrive/MyDrive/cs685/)"
      ],
      "metadata": {
        "id": "gTIyJ7zdbZlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzSrpn5-n-KT",
        "outputId": "3fc613bc-e123-423b-a52a-ef02e5b69a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 2)) (3.4.5)\n",
            "Requirement already satisfied: colorama==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 3)) (0.4.4)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 5)) (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 1)) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (0.9.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r /content/gdrive/MyDrive/cs685/PPLM/requirements.txt (line 4)) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ClassificationHead(torch.nn.Module):\n",
        "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
        "\n",
        "    def __init__(self, class_size, embed_size):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.class_size = class_size\n",
        "        self.embed_size = embed_size\n",
        "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        logits = self.mlp(hidden_state)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "VtRdbhDElIa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Example command with bag of words:\n",
        "python examples/run_pplm.py -B space --cond_text \"The president\" --length 100 --gamma 1.5 --num_iterations 3 --num_samples 10 --stepsize 0.01 --window_length 5 --kl_scale 0.01 --gm_scale 0.95\n",
        "\n",
        "Example command with discriminator:\n",
        "python examples/run_pplm.py -D sentiment --class_label 3 --cond_text \"The lake\" --length 10 --gamma 1.0 --num_iterations 30 --num_samples 10 --stepsize 0.01 --kl_scale 0.01 --gm_scale 0.95\n",
        "# \"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from operator import add\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm import trange\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.file_utils import cached_path"
      ],
      "metadata": {
        "id": "hbBlKR7qlApq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Example command with bag of words:\n",
        "python examples/run_pplm.py -B space --cond_text \"The president\" --length 100 --gamma 1.5 --num_iterations 3 --num_samples 10 --stepsize 0.01 --window_length 5 --kl_scale 0.01 --gm_scale 0.95\n",
        "Example command with discriminator:\n",
        "python examples/run_pplm.py -D sentiment --class_label 3 --cond_text \"The lake\" --length 10 --gamma 1.0 --num_iterations 30 --num_samples 10 --stepsize 0.01 --kl_scale 0.01 --gm_scale 0.95\n",
        "\"\"\"\n",
        "\n",
        "PPLM_BOW = 1\n",
        "# PPLM_DISCRIM = 2\n",
        "# PPLM_BOW_DISCRIM = 3\n",
        "PPLM_BOW_2 = 2\n",
        "PPLM_BOW_1_DISCRIM = 3\n",
        "PPLM_BOW_2_DISCRIM = 4\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n",
        "\n",
        "QUIET = 0\n",
        "REGULAR = 1\n",
        "VERBOSE = 2\n",
        "VERY_VERBOSE = 3\n",
        "VERBOSITY_LEVELS = {\n",
        "    'quiet': QUIET,\n",
        "    'regular': REGULAR,\n",
        "    'verbose': VERBOSE,\n",
        "    'very_verbose': VERY_VERBOSE,\n",
        "}\n",
        "\n",
        "BAG_OF_WORDS_ARCHIVE_MAP_MAIN_TOPIC = {\n",
        "    'legal': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
        "    'military': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
        "    'monsters': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/monsters.txt\",\n",
        "    'politics': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
        "    'positive_words': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/positive_words.txt\",\n",
        "    'religion': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
        "    'science': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
        "    'space': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
        "    'technology': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
        "}\n",
        "\n",
        "BAG_OF_WORDS_ARCHIVE_MAP_SUB_TOPIC = {\n",
        "    'alcohol': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/alcohol.txt\",\n",
        "    'budget': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/budget.txt\",\n",
        "    'business': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/business.txt\",\n",
        "    'crime': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/crime.txt\",\n",
        "    'defense': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/defense.txt\",\n",
        "    'economy': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/economy.txt\",\n",
        "    'education': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/education.txt\",\n",
        "    'elections': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/elections.txt\",\n",
        "    'environment': \"/content/gdrive/MyDrive/cs685/PPLM/bow_topics/environment.txt\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "DISCRIMINATOR_MODELS_PARAMS = {\n",
        "\n",
        "    \"sentiment\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
        "        \"class_size\": 5,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
        "        \"default_class\": 3,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        x = x.cuda()\n",
        "    elif device != 'cuda':\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
        "\n",
        "\n",
        "def top_k_filter(logits, k, probs=False):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins,\n",
        "                               torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins,\n",
        "                           torch.ones_like(logits) * -BIG_CONST,\n",
        "                           logits)\n",
        "\n",
        "\n",
        "def perturb_past(\n",
        "        past,\n",
        "        model,\n",
        "        last,\n",
        "        unpert_past=None,\n",
        "        unpert_logits=None,\n",
        "        accumulated_hidden=None,\n",
        "        grad_norms=None,\n",
        "        stepsize=0.01,\n",
        "        one_hot_bows_vectors_main=None,\n",
        "        one_hot_bows_vectors_sub=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        num_iterations=3,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        kl_scale=0.01,\n",
        "        device='cuda',\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    # Generate inital perturbed past\n",
        "    grad_accumulator = [\n",
        "        (np.zeros(p.shape).astype(\"float32\"))\n",
        "        for p in past\n",
        "    ]\n",
        "\n",
        "    if accumulated_hidden is None:\n",
        "        accumulated_hidden = 0\n",
        "\n",
        "    if decay:\n",
        "        decay_mask = torch.arange(\n",
        "            0.,\n",
        "            1.0 + SMALL_CONST,\n",
        "            1.0 / (window_length)\n",
        "        )[1:]\n",
        "    else:\n",
        "        decay_mask = 1.0\n",
        "\n",
        "    # Generate a mask is gradient perturbated is based on a past window\n",
        "    _, _, _, curr_length, _ = past[0].shape\n",
        "\n",
        "    if curr_length > window_length and window_length > 0:\n",
        "        ones_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        zeros_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([curr_length - window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        ones_mask = torch.ones(ones_key_val_shape)\n",
        "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
        "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
        "\n",
        "        window_mask = torch.cat(\n",
        "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
        "            dim=-2\n",
        "        ).to(device)\n",
        "    else:\n",
        "        window_mask = torch.ones_like(past[0]).to(device)\n",
        "\n",
        "    # accumulate perturbations for num_iterations\n",
        "    loss_per_iter = []\n",
        "    new_accumulated_hidden = None\n",
        "    for i in range(num_iterations):\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(\"Iteration \", i + 1)\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "            for p_ in grad_accumulator\n",
        "        ]\n",
        "\n",
        "        # Compute hidden using perturbed past\n",
        "        perturbed_past = list(map(add, past, curr_perturbation))\n",
        "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
        "        all_logits, _, all_hidden = model(last, past_key_values=perturbed_past)\n",
        "        hidden = all_hidden[-1]\n",
        "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
        "            hidden,\n",
        "            dim=1\n",
        "        ).detach()\n",
        "\n",
        "        logits = all_logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        loss = 0.0\n",
        "        loss_list = []\n",
        "        # if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n",
        "        for one_hot_bow in one_hot_bows_vectors_main:\n",
        "            bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "            bow_loss = -torch.log(torch.sum(bow_logits))\n",
        "            loss += bow_loss\n",
        "            loss_list.append(bow_loss)\n",
        "        if verbosity_level >= VERY_VERBOSE:\n",
        "            print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
        "\n",
        "        if loss_type == PPLM_BOW_2 or loss_type == PPLM_BOW_2_DISCRIM:\n",
        "          for one_hot_bow in one_hot_bows_vectors_sub:\n",
        "            bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "            bow_loss = -torch.log(torch.sum(bow_logits))\n",
        "            loss += bow_loss\n",
        "            loss_list.append(bow_loss)\n",
        "          if verbosity_level >= VERY_VERBOSE:\n",
        "              print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
        "\n",
        "        if loss_type == PPLM_BOW_1_DISCRIM or loss_type == PPLM_BOW_2_DISCRIM:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            curr_unpert_past = unpert_past\n",
        "            curr_probs = torch.unsqueeze(probs, dim=1)\n",
        "            wte = model.resize_token_embeddings()\n",
        "            for _ in range(horizon_length):\n",
        "                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n",
        "                _, curr_unpert_past, curr_all_hidden = model(\n",
        "                    past=curr_unpert_past,\n",
        "                    inputs_embeds=inputs_embeds\n",
        "                )\n",
        "                curr_hidden = curr_all_hidden[-1]\n",
        "                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\n",
        "                    curr_hidden, dim=1)\n",
        "\n",
        "            prediction = classifier(new_accumulated_hidden /\n",
        "                                    (curr_length + 1 + horizon_length))\n",
        "\n",
        "            label = torch.tensor(prediction.shape[0] * [class_label],\n",
        "                                 device=device,\n",
        "                                 dtype=torch.long)\n",
        "            discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())\n",
        "            loss += discrim_loss\n",
        "            loss_list.append(discrim_loss)\n",
        "\n",
        "        kl_loss = 0.0\n",
        "        if kl_scale > 0.0:\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "            unpert_probs = (\n",
        "                    unpert_probs + SMALL_CONST *\n",
        "                    (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
        "            )\n",
        "            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(\n",
        "                device).detach()\n",
        "            corrected_probs = probs + correction.detach()\n",
        "            kl_loss = kl_scale * (\n",
        "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
        "            )\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(' kl_loss', kl_loss.data.cpu().numpy())\n",
        "            loss += kl_loss\n",
        "\n",
        "        loss_per_iter.append(loss.data.cpu().numpy())\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n",
        "\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # calculate gradient norms\n",
        "        if grad_norms is not None and (loss_type == PPLM_BOW or loss_type == PPLM_BOW_2):\n",
        "            grad_norms = [\n",
        "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "        else:\n",
        "            grad_norms = [\n",
        "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "\n",
        "        # normalize gradients\n",
        "        grad = [\n",
        "            -stepsize *\n",
        "            (p_.grad * window_mask / grad_norms[\n",
        "                index] ** gamma).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # accumulate gradient\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # reset gradients, just to make sure\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # removing past from the graph\n",
        "        new_past = []\n",
        "        for p_ in past:\n",
        "            new_past.append(p_.detach())\n",
        "        past = new_past\n",
        "\n",
        "    # apply the accumulated perturbations to the past\n",
        "    grad_accumulator = [\n",
        "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "        for p_ in grad_accumulator\n",
        "    ]\n",
        "    pert_past = list(map(add, past, grad_accumulator))\n",
        "\n",
        "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
        "\n",
        "\n",
        "def get_classifier(\n",
        "        name: Optional[str],\n",
        "        class_label: Union[str, int],\n",
        "        device: str,\n",
        "        verbosity_level: int = REGULAR\n",
        ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
        "    if name is None:\n",
        "        return None, None\n",
        "\n",
        "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
        "    classifier = ClassificationHead(\n",
        "        class_size=params['class_size'],\n",
        "        embed_size=params['embed_size']\n",
        "    ).to(device)\n",
        "    if \"url\" in params:\n",
        "        resolved_archive_file = cached_path(params[\"url\"])\n",
        "    elif \"path\" in params:\n",
        "        resolved_archive_file = params[\"path\"]\n",
        "    else:\n",
        "        raise ValueError(\"Either url or path have to be specified \"\n",
        "                         \"in the discriminator model parameters\")\n",
        "    classifier.load_state_dict(\n",
        "        torch.load(resolved_archive_file, map_location=device))\n",
        "    classifier.eval()\n",
        "\n",
        "    if isinstance(class_label, str):\n",
        "        if class_label in params[\"class_vocab\"]:\n",
        "            label_id = params[\"class_vocab\"][class_label]\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    elif isinstance(class_label, int):\n",
        "        if class_label in set(params[\"class_vocab\"].values()):\n",
        "            label_id = class_label\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    else:\n",
        "        label_id = params[\"default_class\"]\n",
        "\n",
        "    return classifier, label_id\n",
        "\n",
        "\n",
        "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer, bow_level) -> \\\n",
        "        List[List[List[int]]]:\n",
        "    bow_indices = []\n",
        "    if bow_level == \"main\":\n",
        "      BAG_OF_WORDS_ARCHIVE_MAP = BAG_OF_WORDS_ARCHIVE_MAP_MAIN_TOPIC\n",
        "    elif bow_level == \"sub\":\n",
        "      BAG_OF_WORDS_ARCHIVE_MAP = BAG_OF_WORDS_ARCHIVE_MAP_SUB_TOPIC\n",
        "    for id_or_path in bag_of_words_ids_or_paths:\n",
        "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
        "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
        "        else:\n",
        "            filepath = id_or_path\n",
        "        with open(filepath, \"r\") as f:\n",
        "            words = f.read().strip().split(\"\\n\")\n",
        "        bow_indices.append(\n",
        "            [tokenizer.encode(word.strip(),\n",
        "                              add_prefix_space=True,\n",
        "                              add_special_tokens=False)\n",
        "             for word in words])\n",
        "    return bow_indices\n",
        "\n",
        "\n",
        "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
        "    if bow_indices is None:\n",
        "        return None\n",
        "\n",
        "    one_hot_bows_vectors = []\n",
        "    for single_bow in bow_indices:\n",
        "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
        "        single_bow = torch.tensor(single_bow).to(device)\n",
        "        num_words = single_bow.shape[0]\n",
        "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
        "        one_hot_bow.scatter_(1, single_bow, 1)\n",
        "        one_hot_bows_vectors.append(one_hot_bow)\n",
        "    return one_hot_bows_vectors\n",
        "\n",
        "\n",
        "def full_text_generation(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        num_samples=1,\n",
        "        device=\"cuda\",\n",
        "        bag_of_words_main=None,\n",
        "        bag_of_words_sub=None,\n",
        "        discrim=None,\n",
        "        class_label=None,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR,\n",
        "        **kwargs\n",
        "):\n",
        "    classifier, class_id = get_classifier(\n",
        "        discrim,\n",
        "        class_label,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    bow_indices_main = []\n",
        "    if bag_of_words_main:\n",
        "        bow_indices_main = get_bag_of_words_indices(bag_of_words_main.split(\";\"),\n",
        "                                               tokenizer, bow_level = \"main\")\n",
        "    bow_indices_sub = []\n",
        "    if bag_of_words_sub:\n",
        "        bow_indices_sub = get_bag_of_words_indices(bag_of_words_sub.split(\";\"),\n",
        "                                               tokenizer, bow_level = \"sub\")\n",
        "    \n",
        "\n",
        "    if bag_of_words_main and bag_of_words_sub and classifier:\n",
        "        loss_type = PPLM_BOW_2_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Both PPLM-BoW and PPLM-Discrim are on. \"\n",
        "                  \"This is not optimized.\")\n",
        "\n",
        "    if bag_of_words_main and classifier:\n",
        "        loss_type = PPLM_BOW_1_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Both PPLM-BoW and PPLM-Discrim are on. \"\n",
        "                  \"This is not optimized.\")            \n",
        "\n",
        "    elif bag_of_words_main:\n",
        "        loss_type = PPLM_BOW\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-BoW\")\n",
        "\n",
        "    elif bag_of_words_main and bag_of_words_sub:\n",
        "        loss_type = PPLM_BOW_2\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-BoW\")            \n",
        "\n",
        "    # elif classifier is not None:\n",
        "    #     loss_type = PPLM_DISCRIM\n",
        "    #     if verbosity_level >= REGULAR:\n",
        "    #         print(\"Using PPLM-Discrim\")\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Specify bag of words main\")\n",
        "\n",
        "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=context,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        sample=sample,\n",
        "        perturb=False,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    pert_gen_tok_texts = []\n",
        "    discrim_losses = []\n",
        "    losses_in_time = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context=context,\n",
        "            device=device,\n",
        "            perturb=True,\n",
        "            bow_indices_main=bow_indices_main,\n",
        "            bow_indices_sub=bow_indices_sub,\n",
        "            classifier=classifier,\n",
        "            class_label=class_id,\n",
        "            loss_type=loss_type,\n",
        "            length=length,\n",
        "            stepsize=stepsize,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            sample=sample,\n",
        "            num_iterations=num_iterations,\n",
        "            grad_length=grad_length,\n",
        "            horizon_length=horizon_length,\n",
        "            window_length=window_length,\n",
        "            decay=decay,\n",
        "            gamma=gamma,\n",
        "            gm_scale=gm_scale,\n",
        "            kl_scale=kl_scale,\n",
        "            verbosity_level=verbosity_level\n",
        "        )\n",
        "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
        "        if classifier is not None:\n",
        "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
        "        losses_in_time.append(loss_in_time)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "\n",
        "\n",
        "def generate_text_pplm(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        past=None,\n",
        "        device=\"cuda\",\n",
        "        perturb=True,\n",
        "        bow_indices_main=None,\n",
        "        bow_indices_sub=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    output_so_far = None\n",
        "    if context:\n",
        "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
        "        while len(context_t.shape) < 2:\n",
        "            context_t = context_t.unsqueeze(0)\n",
        "        output_so_far = context_t\n",
        "\n",
        "    # collect one hot vectors for bags of words\n",
        "    one_hot_bows_vectors_main = build_bows_one_hot_vectors(bow_indices_main, tokenizer,\n",
        "                                                      device)\n",
        "    one_hot_bows_vectors_sub = build_bows_one_hot_vectors(bow_indices_sub, tokenizer,\n",
        "                                                      device)\n",
        "    grad_norms = None\n",
        "    last = None\n",
        "    unpert_discrim_loss = 0\n",
        "    loss_in_time = []\n",
        "\n",
        "    if verbosity_level >= VERBOSE:\n",
        "        range_func = trange(length, ascii=True)\n",
        "    else:\n",
        "        range_func = range(length)\n",
        "\n",
        "    for i in range_func:\n",
        "\n",
        "        # Get past/probs for current output, except for last word\n",
        "        # Note that GPT takes 2 inputs: past + current_token\n",
        "\n",
        "        # run model forward to obtain unperturbed\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "\n",
        "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
        "        unpert_last_hidden = unpert_all_hidden[-1]\n",
        "\n",
        "        # check if we are abowe grad max length\n",
        "        if i >= grad_length:\n",
        "            current_stepsize = stepsize * 0\n",
        "        else:\n",
        "            current_stepsize = stepsize\n",
        "\n",
        "        # modify the past if necessary\n",
        "        if not perturb or num_iterations == 0:\n",
        "            pert_past = past\n",
        "\n",
        "        else:\n",
        "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
        "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
        "\n",
        "            if past is not None:\n",
        "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
        "                    past,\n",
        "                    model,\n",
        "                    last,\n",
        "                    unpert_past=unpert_past,\n",
        "                    unpert_logits=unpert_logits,\n",
        "                    accumulated_hidden=accumulated_hidden,\n",
        "                    grad_norms=grad_norms,\n",
        "                    stepsize=current_stepsize,\n",
        "                    one_hot_bows_vectors_main=one_hot_bows_vectors_main,\n",
        "                    one_hot_bows_vectors_sub=one_hot_bows_vectors_sub,\n",
        "                    classifier=classifier,\n",
        "                    class_label=class_label,\n",
        "                    loss_type=loss_type,\n",
        "                    num_iterations=num_iterations,\n",
        "                    horizon_length=horizon_length,\n",
        "                    window_length=window_length,\n",
        "                    decay=decay,\n",
        "                    gamma=gamma,\n",
        "                    kl_scale=kl_scale,\n",
        "                    device=device,\n",
        "                    verbosity_level=verbosity_level\n",
        "                )\n",
        "                loss_in_time.append(loss_this_iter)\n",
        "            else:\n",
        "                pert_past = past\n",
        "\n",
        "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
        "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
        "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        if classifier is not None:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
        "            label = torch.tensor([class_label], device=device,\n",
        "                                 dtype=torch.long)\n",
        "            unpert_discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERBOSE:\n",
        "                print(\n",
        "                    \"unperturbed discrim loss\",\n",
        "                    unpert_discrim_loss.data.cpu().numpy()\n",
        "                )\n",
        "        else:\n",
        "            unpert_discrim_loss = 0\n",
        "\n",
        "        # Fuse the modified model and original model\n",
        "        if perturb:\n",
        "\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "\n",
        "            pert_probs = ((pert_probs ** gm_scale) * (\n",
        "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
        "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
        "                                      probs=True)  # + SMALL_CONST\n",
        "\n",
        "            # rescale\n",
        "            if torch.sum(pert_probs) <= 1:\n",
        "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
        "\n",
        "        else:\n",
        "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
        "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        # sample or greedy\n",
        "        if sample:\n",
        "            last = torch.multinomial(pert_probs, num_samples=1)\n",
        "\n",
        "        else:\n",
        "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = (\n",
        "            last if output_so_far is None\n",
        "            else torch.cat((output_so_far, last), dim=1)\n",
        "        )\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(tokenizer.decode(output_so_far.tolist()[0]))\n",
        "\n",
        "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
        "\n",
        "\n",
        "def set_generic_model_params(discrim_weights, discrim_meta):\n",
        "    if discrim_weights is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_weights need to be specified')\n",
        "    if discrim_meta is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_meta need to be specified')\n",
        "\n",
        "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
        "        meta = json.load(discrim_meta_file)\n",
        "    meta['path'] = discrim_weights\n",
        "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta\n",
        "\n",
        "\n",
        "def run_pplm_example(\n",
        "        pretrained_model=\"gpt2-medium\",\n",
        "        cond_text=\"\",\n",
        "        uncond=False,\n",
        "        num_samples=1,\n",
        "        bag_of_words_main=None,\n",
        "        bag_of_words_sub=None,\n",
        "        discrim=None,\n",
        "        discrim_weights=None,\n",
        "        discrim_meta=None,\n",
        "        class_label=-1,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        seed=0,\n",
        "        no_cuda=False,\n",
        "        colorama=False,\n",
        "        verbosity='regular'\n",
        "):\n",
        "    # set Random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # set verbosiry\n",
        "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
        "\n",
        "    # set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "\n",
        "    if discrim == 'generic':\n",
        "        set_generic_model_params(discrim_weights, discrim_meta)\n",
        "\n",
        "    if discrim is not None:\n",
        "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
        "            \"pretrained_model\"\n",
        "        ]\n",
        "        if pretrained_model != discriminator_pretrained_model:\n",
        "            pretrained_model = discriminator_pretrained_model\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"discrim = {}, pretrained_model set \"\n",
        "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
        "\n",
        "    # load pretrained model\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        pretrained_model,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # figure out conditioning text\n",
        "    if uncond:\n",
        "        tokenized_cond_text = tokenizer.encode(\n",
        "            [tokenizer.bos_token],\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "    else:\n",
        "        raw_text = cond_text\n",
        "        while not raw_text:\n",
        "            print(\"Did you forget to add `--cond_text`? \")\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "        tokenized_cond_text = tokenizer.encode(\n",
        "            tokenizer.bos_token + raw_text,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "    \n",
        "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=tokenized_cond_text,\n",
        "        device=device,\n",
        "        num_samples=num_samples,\n",
        "        bag_of_words_main=bag_of_words_main,\n",
        "        bag_of_words_sub=bag_of_words_sub,\n",
        "        discrim=discrim,\n",
        "        class_label=class_label,\n",
        "        length=length,\n",
        "        stepsize=stepsize,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        sample=sample,\n",
        "        num_iterations=num_iterations,\n",
        "        grad_length=grad_length,\n",
        "        horizon_length=horizon_length,\n",
        "        window_length=window_length,\n",
        "        decay=decay,\n",
        "        gamma=gamma,\n",
        "        gm_scale=gm_scale,\n",
        "        kl_scale=kl_scale,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "    generated_text_ = []\n",
        "    # untokenize unperturbed text\n",
        "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
        "\n",
        "    if verbosity_level >= REGULAR:\n",
        "        print(\"=\" * 80)\n",
        "    print(\"= Unperturbed generated text =\")\n",
        "    print(unpert_gen_text)\n",
        "    # generated_text_.append(unpert_gen_text)\n",
        "    print()\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    bow_word_ids_main = set()\n",
        "    if bag_of_words_main and colorama:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words_main.split(\";\"),\n",
        "                                               tokenizer)\n",
        "        for single_bow_list in bow_indices:\n",
        "            # filtering all words in the list composed of more than 1 token\n",
        "            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n",
        "            # w[0] because we are sure w has only 1 item because previous fitler\n",
        "            bow_word_ids_main.update(w[0] for w in filtered)\n",
        "\n",
        "    bow_word_ids_sub = set()\n",
        "    if bag_of_words_sub and colorama:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words_sub.split(\";\"),\n",
        "                                               tokenizer)\n",
        "        for single_bow_list in bow_indices:\n",
        "            # filtering all words in the list composed of more than 1 token\n",
        "            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n",
        "            # w[0] because we are sure w has only 1 item because previous fitler\n",
        "            bow_word_ids_sub.update(w[0] for w in filtered)\n",
        "\n",
        "    # iterate through the perturbed texts\n",
        "    \n",
        "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
        "        try:\n",
        "            # untokenize unperturbed text\n",
        "            if colorama:\n",
        "                import colorama\n",
        "\n",
        "                pert_gen_text = ''\n",
        "                for word_id in pert_gen_tok_text.tolist()[0]:\n",
        "                    if (word_id in bow_word_ids_main) or (word_id in bow_word_ids_sub):\n",
        "                        pert_gen_text += '{}{}{}'.format(\n",
        "                            colorama.Fore.RED,\n",
        "                            tokenizer.decode([word_id]),\n",
        "                            colorama.Style.RESET_ALL\n",
        "                        )\n",
        "                    else:\n",
        "                        pert_gen_text += tokenizer.decode([word_id])\n",
        "            else:\n",
        "                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
        "\n",
        "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
        "            print(pert_gen_text)\n",
        "            generated_text_.append(pert_gen_text)\n",
        "            print()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # keep the prefix, perturbed seq, original seq for each index\n",
        "        generated_texts.append(\n",
        "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
        "        )\n",
        "\n",
        "    return generated_text_\n",
        "\n"
      ],
      "metadata": {
        "id": "lmyY30VW0H4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "prompts_samples = pd.read_csv(\"/content/additional_prompts_approach1.csv\")\n",
        "print(prompts_samples.shape)\n",
        "prompts_samples.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "b-HFtsw0ETr6",
        "outputId": "75ac9f8a-75b2-48ef-fae8-8d03ec5a6ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I stand here today humbled by the task before ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Today I say to you that the challenges we face...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I want to thank my partner in this journey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I welcome you all to this grand event</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Good morning, ladies and gentlemen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt\n",
              "0  I stand here today humbled by the task before ...\n",
              "1  Today I say to you that the challenges we face...\n",
              "2         I want to thank my partner in this journey\n",
              "3              I welcome you all to this grand event\n",
              "4                 Good morning, ladies and gentlemen"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "list_generated_text = []\n",
        "with open(\"/content/gdrive/MyDrive/cs685/pplm_approach1_economy_discrim_stepsizelow_.csv\", \"a+\") as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  for prompt in prompts_samples['prompt']:\n",
        "    print(prompt)\n",
        "    generated_text_pplm = run_pplm_example(\n",
        "        cond_text=prompt,\n",
        "        num_samples=1,\n",
        "        bag_of_words_main='politics',\n",
        "        bag_of_words_sub=\"economy\",\n",
        "        discrim='generic',\n",
        "        discrim_meta=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_meta.json\",\n",
        "        discrim_weights=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_epoch_5.pt\",\n",
        "        class_label=0,\n",
        "        length=100,\n",
        "        stepsize=0.01,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        window_length=5,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.95,\n",
        "        kl_scale=0.01,\n",
        "        verbosity='quiet'\n",
        "    )\n",
        "    list_generated_text.append(pd.Series([prompt,generated_text_pplm]))\n",
        "    csvwriter.writerows(list_generated_text)"
      ],
      "metadata": {
        "id": "9lrBYbCw1r17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text_pplm = run_pplm_example(\n",
        "    cond_text=\"mr. speaker. i rise today to honor a true virginian and a great american. congressman norm sisisky. congressman sisisky has\",\n",
        "    num_samples=1,\n",
        "    bag_of_words_main='politics',\n",
        "    bag_of_words_sub=\"economy\",\n",
        "    discrim='generic',\n",
        "    discrim_meta=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_meta.json\",\n",
        "    discrim_weights=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_epoch_5.pt\",\n",
        "    class_label=0,\n",
        "    length=100,\n",
        "    stepsize=0.001,\n",
        "    sample=True,\n",
        "    num_iterations=3,\n",
        "    window_length=5,\n",
        "    gamma=1.5,\n",
        "    gm_scale=0.95,\n",
        "    kl_scale=0.01,\n",
        "    verbosity='quiet'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y47suRzeDQZo",
        "outputId": "8aa2f5bb-ea50-4d0d-8c43-d89db1694206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_gpt2.py:759: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= Unperturbed generated text =\n",
            "<|endoftext|>mr. speaker. i rise today to honor a true virginian and a great american. congressman norm sisisky. congressman sisisky has been on our radar for some time now, but has remained largely hidden. i have been hearing about him from friends and family. he has been a wonderful person for many years. he has made his peace with a lot of his decisions, and his life has been a lot simpler than what others thought it could be. i know his wife and two children. his wife is a beautiful woman who loves her children more than life itself. her children are both very intelligent. her husband is a man who\n",
            "\n",
            "= Perturbed generated text 1 =\n",
            "<|endoftext|>mr. speaker. i rise today to honor a true virginian and a great american. congressman norm sisisky. congressman sisisky has spent over a decade as a member of congress in his home town of lafayette. he is an excellent example to follow and i am here to honor him. i was born to a single mother and was raised by two fathers; one was a pastor, the other was a police officer. i was born to a single mom and raised by a single father; one was a pastor and the other one was a cop. i love this congress because of all that it has done for my people;\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_pplm_example(\n",
        "    cond_text=\"It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here. \",\n",
        "    # cond_text=\"The country\",\n",
        "    num_samples=3,\n",
        "    bag_of_words_main='politics',\n",
        "    bag_of_words_sub=\"business\",\n",
        "    # discrim='generic',\n",
        "    # discrim_meta=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_meta.json\",\n",
        "    # discrim_weights=\"/content/gdrive/MyDrive/cs685/classifier_model/party/generic_classifier_head_epoch_5.pt\",\n",
        "    class_label=0,\n",
        "    length=100,\n",
        "    stepsize=0.005,\n",
        "    sample=True,\n",
        "    num_iterations=3,\n",
        "    window_length=5,\n",
        "    gamma=1.5,\n",
        "    gm_scale=0.95,\n",
        "    kl_scale=0.01,\n",
        "    verbosity='quiet',\n",
        "    # generated_text\n",
        ")"
      ],
      "metadata": {
        "id": "ygHA3NPa5F3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912ca379-f9e8-4105-ed46-a04d7136e957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_gpt2.py:759: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= Unperturbed generated text =\n",
            "<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here.  I am glad that our chairman introduced it to us.  It is good to know that you are taking these matters very seriously and I am pleased that you are working with the chairman and the ranking member to get this done.  I thank Chairman Burr and Ranking Member Burr for bringing it to us.  They have been very helpful in this process.  I look forward to this hearing today.  Thank you all for being here today.  We appreciate your time\n",
            "\n",
            "= Perturbed generated text 1 =\n",
            "<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here.  We are pleased to report that Senator Graham has received his written and verbal commitments for the legislation.  In fact, he will be voting this evening.  We thank the Chairman of the Judiciary Committee, Senator Grassley, and our ranking member, Senator Collins for their leadership on this important bill.  We thank the staff of the Committee on Homeland Security and Governmental Affairs and Senator Grassley and Senators Graham and Collins for the hard work that has gone into crafting this bill.  We\n",
            "\n",
            "= Perturbed generated text 2 =\n",
            "<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here.  It is important because it will provide a much better framework of law for dealing with the issue than has been proposed at this stage. We are concerned about the lack of transparency.  There is no way to know exactly what is being collected, how much it is being collected and where that money is coming from.  This bill will provide a much clearer system of oversight to ensure that we are paying what we owe.  We are asking the House to vote this day because we want\n",
            "\n",
            "= Perturbed generated text 3 =\n",
            "<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here.  The House of Representatives has a right and obligation in its constitutional role to consider and consider and consider the recommendations contained in this bill and make them law.  We cannot allow this legislation to be voted on without it having been reviewed, examined, and considered by our committees and by the House of Representatives.\n",
            "The legislation is very well thought through, and it provides a great deal of relief to families and individuals who face the prospect of losing their benefits. This bill provides relief that is not available\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here. \\xa0I am glad that our chairman introduced it to us. \\xa0It is good to know that you are taking these matters very seriously and I am pleased that you are working with the chairman and the ranking member to get this done. \\xa0I thank Chairman Burr and Ranking Member Burr for bringing it to us. \\xa0They have been very helpful in this process. \\xa0I look forward to this hearing today. \\xa0Thank you all for being here today. \\xa0We appreciate your time',\n",
              " '<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here. \\xa0We are pleased to report that Senator Graham has received his written and verbal commitments for the legislation. \\xa0In fact, he will be voting this evening. \\xa0We thank the Chairman of the Judiciary Committee, Senator Grassley, and our ranking member, Senator Collins for their leadership on this important bill. \\xa0We thank the staff of the Committee on Homeland Security and Governmental Affairs and Senator Grassley and Senators Graham and Collins for the hard work that has gone into crafting this bill. \\xa0We',\n",
              " '<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here. \\xa0It is important because it will provide a much better framework of law for dealing with the issue than has been proposed at this stage. We are concerned about the lack of transparency. \\xa0There is no way to know exactly what is being collected, how much it is being collected and where that money is coming from. \\xa0This bill will provide a much clearer system of oversight to ensure that we are paying what we owe. \\xa0We are asking the House to vote this day because we want',\n",
              " '<|endoftext|>It is vital this bill be before us. and I congratulate our chairman and our ranking member for bringing it here. \\xa0The House of Representatives has a right and obligation in its constitutional role to consider and consider and consider the recommendations contained in this bill and make them law. \\xa0We cannot allow this legislation to be voted on without it having been reviewed, examined, and considered by our committees and by the House of Representatives.\\nThe legislation is very well thought through, and it provides a great deal of relief to families and individuals who face the prospect of losing their benefits. This bill provides relief that is not available']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ds_23PONeThj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}